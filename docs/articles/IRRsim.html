<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Relationship Between Intraclass Correlation (ICC) and Percent Agreement • IRRsim</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Relationship Between Intraclass Correlation (ICC) and Percent Agreement">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">IRRsim</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">1.0.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/IRRsim.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="http://github.com/jbryer/IRRsim">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Relationship Between Intraclass Correlation (ICC) and Percent Agreement</h1>
                        <h4 class="author">Jason Bryer</h4>
            <address class="author_afil">
      Excelsior College<br><h4 class="author">Guher Gorgun</h4>
            <address class="author_afil">
      University at Albany<br><h4 class="date">2018-11-29</h4>
      
      <small class="dont-index">Source: <a href="http://github.com/jbryer/IRRsim/blob/master/vignettes/IRRsim.Rmd"><code>vignettes/IRRsim.Rmd</code></a></small>
      <div class="hidden name"><code>IRRsim.Rmd</code></div>

    </address>
</address>
</div>

    
        <div class="abstract">
      <p class="abstract">Abstract</p>
      Inter-rater reliability (IRR) is a critical component of establishing the reliability of measures when more than one rater is necessary. There are numerous IRR statistics available to researchers including percent rater agreement, Cohen’s Kappa, and several types of intraclass correlations (ICC). Several methodologists suggest using ICC over percent rater agreement (Hallgren, 2012; Koo &amp; Li, 2016; McGraw &amp; Wong, 1996; Shrout &amp; Fleiss, 1979). However, the literature provides little guidance on the interpretation of ICC results. This article explores the relationship between ICC and percent rater agreement using simulations. Results suggest that ICC and percent rater agreement are highly correlated (R² &gt; 0.9) for most designs.
    </div>
    
<p>When raters are involved in scoring procedures, inter-rater reliability (IRR) measures are used to establish the reliability of measures. Commonly used IRR measures include percent rater agreement, intraclass correlation coefficients (ICC), and Cohen’s Kappa. Several researchers recommend using ICC and Cohen’s Kappa over Percent Agreement (Hallgren, 2012; Koo &amp; Li, 2016; McGraw &amp; Wong, 1996; Shrout &amp; Fleiss, 1979). However, there are misconceptions and inconsistencies when it comes to proper application, interpretation, and reporting of these measures (Kottner et al., 2011; &amp; Trevethan, 2017). Moreover, researchers tend to recommend different thresholds for poor, moderate, and good level of reliability (see Table 2). These inconsistencies, and the paucity of detailed reports of test methods and results, perpetuate the misconceptions in the application and interpretation of IRR measures.</p>
<p>Current recommendations regarding the thresholds of reliability estimates suggest considering purposes and consequences of tests, and the magnitude of error allowed in test interpretation and decision making (Trevethan, 2017; AERA, NCME, &amp; APA, 2014; Kottner at al., 2011). Furthermore, Kottner et al. (2011) also recommend reporting multiple reliability estimates. A low ICC might be due to lack of variability between subjects so by reporting different reliability coefficients (e.g. percent agreement) readers can get a more complete understanding of the degree of reliability.</p>
<p>Table 1. <em>Descriptions and formulas of IRR measures</em></p>
<table class="table">
<colgroup>
<col width="25%">
<col width="30%">
<col width="44%">
</colgroup>
<thead><tr class="header">
<th align="left">IRR Statistic</th>
<th align="left">Description</th>
<th align="center">Formula</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Percent Agreement</td>
<td align="left">One-way random effects; Absolute agreement</td>
<td align="center"><span class="math inline">\(\frac{number\ of\ observations\ agreed\ upon}{total\ number\ of\ observations}\)</span></td>
</tr>
<tr class="even">
<td align="left">ICC(1,1)</td>
<td align="left">One-way random effects; absolute agreement; single measurements</td>
<td align="center"><span class="math inline">\(\frac{MS_R - MS_W}{MS_R + (k - 1)MS_W}\)</span></td>
</tr>
<tr class="odd">
<td align="left">ICC(2,1)</td>
<td align="left">Two-way random effects; absolute agreement; single measures</td>
<td align="center"><span class="math inline">\(\frac{MS_R - MS_W}{MS_R + (k - 1)MS_E + \frac{k}{n}(MS_C - MS_E)}\)</span></td>
</tr>
<tr class="even">
<td align="left">ICC(3,1)</td>
<td align="left">Two-way random mixed effects; consistency; single measures.</td>
<td align="center"><span class="math inline">\(\frac{MS_R - MS_E}{MS_R + (k-1)MS_E}\)</span></td>
</tr>
<tr class="odd">
<td align="left">ICC(1,<em>k</em>)</td>
<td align="left">One-way random effects; absolute agreement; average measures.</td>
<td align="center"><span class="math inline">\(\frac{MS_R - MS_W}{MS_R}\)</span></td>
</tr>
<tr class="even">
<td align="left">ICC(2,<em>k</em>)</td>
<td align="left">Two-way random effects; absolute agreement; average measures.</td>
<td align="center"><span class="math inline">\(\frac{MS_R - MS_E}{MS_R | \frac{MS_C - MS_E}{n}}\)</span></td>
</tr>
<tr class="odd">
<td align="left">ICC(3,<em>k</em>)</td>
<td align="left">Two-way mixed effects; consistency; average measures.</td>
<td align="center"><span class="math inline">\(\frac{MS_R - MS_E}{MS_R}\)</span></td>
</tr>
<tr class="even">
<td align="left">Cohen’s Kappa (κ)</td>
<td align="left">Absolute agreement</td>
<td align="center"><span class="math inline">\(\frac{P_o - P_e}{1 - P_e}\)</span></td>
</tr>
</tbody>
</table>
<p><em>Note.</em> <span class="math inline">\(MS_R\)</span> = mean square for rows; <span class="math inline">\(MS_W\)</span> = mean square for risudal sources of variance; <span class="math inline">\(MS_E\)</span> = mean square error; <span class="math inline">\(MS_C\)</span> = mean square for columns; <span class="math inline">\(P_o\)</span> = observed agreement rates; <span class="math inline">\(P_e\)</span> = expected agreement rates.</p>
<p>Table 2. <em>Guidelines for IRR estimates</em></p>
<table class="table">
<thead><tr class="header">
<th></th>
<th align="left">Reference</th>
<th align="left">IRRMetric</th>
<th align="left">Guidelines</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Altman</td>
<td align="left">Altman (1990)</td>
<td align="left"></td>
<td align="left">&lt; 0.2 Poor<br>0.2 - 0.4 Fair<br>0.4 - 0.6 Moderate<br>0.6 - 0.8 Good<br>&gt; 0.8 Very good</td>
</tr>
<tr class="even">
<td>Cicchetti</td>
<td align="left">Cicchetti &amp; Sparrow (1981); Cicchetti (2001)</td>
<td align="left">ICC, Cohen Kappa</td>
<td align="left">&lt; 0.4 Poor<br>0.4 - 0.6 Fair<br>0.6 - 0.75 Good<br>&gt; 0.75 Excellent</td>
</tr>
<tr class="odd">
<td>Fleiss</td>
<td align="left">Fleiss (1981, 1986); Brage et al. (1998); Martin et al. (1997); Svanholm et al. (1989)</td>
<td align="left">Cohen Kappa</td>
<td align="left">&lt; 0.4 Poor<br>0.4 - 0.75 Fair<br>&gt; 0.75 Excellent</td>
</tr>
<tr class="even">
<td>Koo and Li</td>
<td align="left">Koo &amp; Li (2016)</td>
<td align="left">ICC</td>
<td align="left">&lt; 0.5 Poor<br>0.5 - 0.75 Moderate<br>0.75 - 0.9 Good<br>&gt; 0.9 Excellent</td>
</tr>
<tr class="odd">
<td>Landis and Koch</td>
<td align="left">Landis &amp; Koch (1997); Zeger et al. (2010)</td>
<td align="left">Cohen Kappa</td>
<td align="left">&lt; 0.2 Slight<br>0.2 - 0.4 Fair<br>0.4 - 0.6 Moderate<br>0.6 - 0.8 Substantial<br>&gt; 0.8 Almost perfect</td>
</tr>
<tr class="even">
<td>Portney and Watkins</td>
<td align="left">Portney &amp; Watkins (2009)</td>
<td align="left">ICC</td>
<td align="left">&lt; 0.75 Poor to moderate<br>0.75 - NA Reasonable for clinical measurement<br>0 - 0.75 Poor to moderate<br>&gt; 0.75 Reasonable for clinical measurement</td>
</tr>
<tr class="odd">
<td>Shrout</td>
<td align="left">Shrout (1998)</td>
<td align="left"></td>
<td align="left">&lt; 0.1 Virtually none<br>0.1 - 0.4 Slight<br>0.4 - 0.6 Fair<br>0.6 - 0.8 Moderate<br>&gt; 0.8 Substantial</td>
</tr>
</tbody>
</table>
<div id="research-questions" class="section level1">
<h1 class="hasAnchor">
<a href="#research-questions" class="anchor"></a>Research Questions</h1>
<p>Given the different types of ICC and guidelines for interpretation, this paper is guided by the following research questions:</p>
<ol style="list-style-type: decimal">
<li>What is the relationship between ICC and PRA?</li>
<li>Are the published guidelines for interpreting ICC appropriate for all rating designs?</li>
</ol>
</div>
<div id="method" class="section level1">
<h1 class="hasAnchor">
<a href="#method" class="anchor"></a>Method</h1>
<p>There are several designs for establishing IRR. We are generally concerned with the ratings of <strong>m</strong> subjects by <strong>k</strong> raters. The simplest design is <span class="math inline">\(m x 2\)</span> where two raters score all <strong>m</strong> subjects. However, it is common in education to have <span class="math inline">\(k &gt; 2\)</span> raters where each subject is scored by <span class="math inline">\(k_m = 2\)</span> resulting in a <strong>sparce</strong> matrix. Shrout and Fleiss (1979) provide guidance on which of the six types of ICC to use depending on your design. The six types of ICC considered here are:</p>
<ul>
<li>
<em>ICC1</em> - Each subject is rated by 2 random raters.</li>
<li>
<em>ICC2</em> - Each subject is rated by <span class="math inline">\(k_m &gt; 2\)</span> raters.</li>
<li>
<em>ICC3</em> - Each subject is rated by a fixed set of <em>k</em> raters.</li>
<li>
<em>ICC1k</em> - 2-way ANOVA version of ICC1.</li>
<li>
<em>ICC2k</em> - 2-way ANOVA version of ICC2.</li>
<li>
<em>ICC3k</em> - 2-way ANOVA version of ICC3.</li>
</ul>
<p>The <code>IRRsim</code> package implements several functions to facilitate simulating various scoring designs. To begin, let’s look at the <code>simulateRatingMatrix</code> function. This function will generate an m x k matrix. The algorithm works as follows:</p>
<ol style="list-style-type: decimal">
<li>For each row one rater is selected at random.</li>
<li>For that rater, a score is randomly selected from the distribution defined by the <code>response.probs</code> paramter. By default, values are selected from a uniform distribution.</li>
<li>A random number between zero and one is generated. If that number is less than or equal to the desired agreement as defined by the <code>agree</code> parameter, the score for the remaining raters is set equal to the score from step two. Otherwise, scores are randomly selected for the remaining raters using the distribution defined by the <code>response.probs</code> parameter.</li>
<li>If <span class="math inline">\(k_m &lt; k\)</span>, then <span class="math inline">\(k - k_m\)</span> raters from each row are selected and their scores are deleted (specifically, set to <code>NA</code>).</li>
</ol>
<p>The following example demonstrates creating a 10 x 6 scoring matrix with four scoring levels and a desired percent rater agreement of 60%.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2112</span>)
test1 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/simulateRatingMatrix.html">simulateRatingMatrix</a></span>(<span class="dt">nLevels =</span> <span class="dv">4</span>, 
                              <span class="dt">k =</span> <span class="dv">6</span>, 
                              <span class="dt">k_per_event =</span> <span class="dv">6</span>, 
                              <span class="dt">agree =</span> <span class="fl">0.6</span>, 
                              <span class="dt">nEvents =</span> <span class="dv">10</span>)
test1
<span class="co">#&gt;    aa ab ac ad ae af</span>
<span class="co">#&gt; 1   1  1  1  1  1  1</span>
<span class="co">#&gt; 2   4  4  4  4  4  4</span>
<span class="co">#&gt; 3   3  3  4  4  4  2</span>
<span class="co">#&gt; 4   4  1  4  3  1  3</span>
<span class="co">#&gt; 5   2  2  2  2  2  2</span>
<span class="co">#&gt; 6   2  2  2  2  2  2</span>
<span class="co">#&gt; 7   2  2  3  1  2  4</span>
<span class="co">#&gt; 8   1  3  3  1  4  2</span>
<span class="co">#&gt; 9   4  4  4  2  2  3</span>
<span class="co">#&gt; 10  4  4  4  4  4  4</span></code></pre></div>
<p>In many educational contexts, <span class="math inline">\(k_m = 2\)</span>. The following example simulates the same scoring matrix but retains only two scores per scoring event.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2112</span>)
test2 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/simulateRatingMatrix.html">simulateRatingMatrix</a></span>(<span class="dt">nLevels =</span> <span class="dv">4</span>, 
                              <span class="dt">k =</span> <span class="dv">6</span>, 
                              <span class="dt">k_per_event =</span> <span class="dv">2</span>,
                              <span class="dt">agree =</span> <span class="fl">0.6</span>, 
                              <span class="dt">nEvents =</span> <span class="dv">10</span>)
test2
<span class="co">#&gt;    aa ab ac ad ae af</span>
<span class="co">#&gt; 1   1 NA  1 NA NA NA</span>
<span class="co">#&gt; 2   4 NA NA NA  4 NA</span>
<span class="co">#&gt; 3   3 NA NA  4 NA NA</span>
<span class="co">#&gt; 4  NA  1 NA NA NA  3</span>
<span class="co">#&gt; 5  NA  2  2 NA NA NA</span>
<span class="co">#&gt; 6  NA  2 NA NA  2 NA</span>
<span class="co">#&gt; 7   2 NA NA NA  2 NA</span>
<span class="co">#&gt; 8  NA NA  3 NA  4 NA</span>
<span class="co">#&gt; 9   4  4 NA NA NA NA</span>
<span class="co">#&gt; 10 NA  4  4 NA NA NA</span></code></pre></div>
<p>The <code>agreement</code> function calculates the percent rater agreement for an indiviual scoring matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/agreement.html">agreement</a></span>(test1)
<span class="co">#&gt; [1] 0.5</span>
<span class="kw"><a href="../reference/agreement.html">agreement</a></span>(test2)
<span class="co">#&gt; [1] 0.7</span></code></pre></div>
<p>To examine the relationship between percent rater agreement and other inter-rater reliability statistics, we will simulate many scoring matrices with percent-rater agreements spanning the full range of values from 0% to 100%. The <code>simulateIRR</code> utilizes the <code>simulateRatingMatrix</code> for generating many scoring matrices for varying percent rater agreements.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">test3 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/simulateIRR.html">simulateIRR</a></span>(<span class="dt">nLevels =</span> <span class="dv">4</span>,
                     <span class="dt">nRaters =</span> <span class="dv">6</span>,
                     <span class="dt">nEvents =</span> <span class="dv">100</span>,
                     <span class="dt">nSamples =</span> <span class="dv">10</span>,
                     <span class="dt">parallel =</span> <span class="ot">FALSE</span>,
                     <span class="dt">showTextProgress =</span> <span class="ot">FALSE</span>)
<span class="kw">plot</span>(test3)</code></pre></div>
<p><img src="IRRsim_files/figure-html/simulateIRRexample-1.png" width="624"></p>
<p>For the remainder of the document, we wish to estimate ICC for 6, 9, and 12 raters under the conditions of 3, 5, and 9 scoring levels.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tests.4levels &lt;-<span class="st"> </span><span class="kw"><a href="../reference/simulateIRR.html">simulateIRR</a></span>(<span class="dt">nRaters =</span> <span class="kw">c</span>(<span class="dv">6</span>, <span class="dv">9</span>, <span class="dv">12</span>), <span class="dt">nLevels =</span> <span class="dv">4</span>)</code></pre></div>
<p>The <code>as.data.frame</code> function provides the IRR statistics for each of the simulated samples.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">test.4levels.df &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(tests.4levels)
<span class="kw">dim</span>(test.4levels.df)
<span class="co">#&gt; [1] 2700   15</span>
<span class="kw">head</span>(test.4levels.df)
<span class="co">#&gt;   nLevels nEvents k k_per_event simAgreement agreement skewness kurtosis</span>
<span class="co">#&gt; 1       4     100 6           6          0.1      0.09   -0.035     -1.4</span>
<span class="co">#&gt; 2       4     100 6           6          0.1      0.13    0.027     -1.4</span>
<span class="co">#&gt; 3       4     100 6           6          0.1      0.11   -0.049     -1.4</span>
<span class="co">#&gt; 4       4     100 6           6          0.1      0.05    0.097     -1.4</span>
<span class="co">#&gt; 5       4     100 6           6          0.1      0.06    0.062     -1.3</span>
<span class="co">#&gt; 6       4     100 6           6          0.1      0.10   -0.132     -1.3</span>
<span class="co">#&gt;   MaxResponseDiff  ICC1  ICC2  ICC3 ICC1k ICC2k ICC3k</span>
<span class="co">#&gt; 1           0.038 0.108 0.107 0.106 0.420 0.417 0.415</span>
<span class="co">#&gt; 2           0.032 0.114 0.113 0.112 0.435 0.433 0.431</span>
<span class="co">#&gt; 3           0.027 0.150 0.150 0.150 0.515 0.514 0.514</span>
<span class="co">#&gt; 4           0.000 0.024 0.024 0.024 0.126 0.128 0.129</span>
<span class="co">#&gt; 5           0.020 0.015 0.014 0.014 0.081 0.076 0.076</span>
<span class="co">#&gt; 6           0.067 0.058 0.059 0.059 0.270 0.272 0.272</span></code></pre></div>
<p>The following three figures show ICC1 against percent rater agreement.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(tests.4levels, <span class="dt">stat =</span> <span class="st">'ICC1'</span>)</code></pre></div>
<p><img src="IRRsim_files/figure-html/ploticc1-1.png" width="624"></p>
<p>The figure below show the relationship between percent rater agreement and intraclass correlation for the six types of ICC.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(tests.4levels)</code></pre></div>
<p><img src="IRRsim_files/figure-html/plotall-1.png" width="624"></p>
<p>The literature suggests that ICC be used over percent agreement. However, the figures above suggest there is a strong relationship between percent rater agreement and ICC. Below, we fit a quadratic model predicting ICC1 from percent agreement. In all three cases the resulting <span class="math inline">\(R^2\)</span> is greater than .9!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tests.4levels.sum &lt;-<span class="st"> </span><span class="kw">summary</span>(tests.4levels, <span class="dt">stat =</span> <span class="st">'ICC1'</span>, <span class="dt">method =</span> <span class="st">'quadratic'</span>)
<span class="kw">summary</span>(tests.4levels.sum<span class="op">$</span>model[[<span class="dv">1</span>]]) <span class="co"># k = 6 raters</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = as.formula(paste0(stat, " ~ I(agreement^2) + agreement")), </span>
<span class="co">#&gt;     data = test)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residuals:</span>
<span class="co">#&gt;      Min       1Q   Median       3Q      Max </span>
<span class="co">#&gt; -0.12629 -0.02074 -0.00025  0.02233  0.10363 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;                Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="co">#&gt; (Intercept)    0.000883   0.004157    0.21     0.83    </span>
<span class="co">#&gt; I(agreement^2) 0.021894   0.018480    1.18     0.24    </span>
<span class="co">#&gt; agreement      0.979926   0.019009   51.55   &lt;2e-16 ***</span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residual standard error: 0.033 on 897 degrees of freedom</span>
<span class="co">#&gt; Multiple R-squared:  0.984,  Adjusted R-squared:  0.984 </span>
<span class="co">#&gt; F-statistic: 2.77e+04 on 2 and 897 DF,  p-value: &lt;2e-16</span>
<span class="kw">summary</span>(tests.4levels.sum<span class="op">$</span>model[[<span class="dv">2</span>]]) <span class="co"># k = 9 raters</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = as.formula(paste0(stat, " ~ I(agreement^2) + agreement")), </span>
<span class="co">#&gt;     data = test)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residuals:</span>
<span class="co">#&gt;      Min       1Q   Median       3Q      Max </span>
<span class="co">#&gt; -0.11869 -0.01638  0.00095  0.01687  0.09557 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;                Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="co">#&gt; (Intercept)     0.00280    0.00336    0.83     0.41    </span>
<span class="co">#&gt; I(agreement^2)  0.01558    0.01490    1.05     0.30    </span>
<span class="co">#&gt; agreement       0.98333    0.01534   64.11   &lt;2e-16 ***</span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residual standard error: 0.027 on 897 degrees of freedom</span>
<span class="co">#&gt; Multiple R-squared:  0.989,  Adjusted R-squared:  0.989 </span>
<span class="co">#&gt; F-statistic: 4.1e+04 on 2 and 897 DF,  p-value: &lt;2e-16</span>
<span class="kw">summary</span>(tests.4levels.sum<span class="op">$</span>model[[<span class="dv">3</span>]]) <span class="co"># k = 12 raters</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = as.formula(paste0(stat, " ~ I(agreement^2) + agreement")), </span>
<span class="co">#&gt;     data = test)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residuals:</span>
<span class="co">#&gt;      Min       1Q   Median       3Q      Max </span>
<span class="co">#&gt; -0.12831 -0.01676  0.00048  0.01737  0.08210 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt;                 Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="co">#&gt; (Intercept)    -0.005104   0.003522   -1.45     0.15    </span>
<span class="co">#&gt; I(agreement^2) -0.000161   0.015734   -0.01     0.99    </span>
<span class="co">#&gt; agreement       1.006433   0.016152   62.31   &lt;2e-16 ***</span>
<span class="co">#&gt; ---</span>
<span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Residual standard error: 0.029 on 897 degrees of freedom</span>
<span class="co">#&gt; Multiple R-squared:  0.988,  Adjusted R-squared:  0.988 </span>
<span class="co">#&gt; F-statistic: 3.76e+04 on 2 and 897 DF,  p-value: &lt;2e-16</span></code></pre></div>
</div>
<div id="discussion" class="section level1">
<h1 class="hasAnchor">
<a href="#discussion" class="anchor"></a>Discussion</h1>
<p>Methodologists have consistently argued that ICC is preferred over PRA for reporting inter- rater reliability (Hallgren, 2012; Koo &amp; Li, 2016; McGraw &amp; Wong, 1996; Shrout &amp; Fleiss, 1979). Although some recommendations for interpreting ICC have been given (Table 2), the form of ICC (Table 1) those recommendations apply to has not specified by the authors. Furthermore, the nature of the design, especially with regard to the number of possible raters, has substantial impact on the magnitude of ICC (Figure 2). For example, all other things kept equal, increasing the design from 2 to 12 raters changes the required PRA from 61% to 91% to achieve Cicchitti’s (2001) “fair” threshold. And with eight or more raters, “good” or “excellent” reliability are not even possible under this design.</p>
<p>We concur with Kottner et al (2011) and Koo and Li (2016) recommendation that the design features along with multiple IRR statistics be reported by researchers. Given the ease of interpretability of PRA, this may be a desirable metric during the rating process. To assist researchers on interpreting ICC in relation to PRA, we have developed an R Shiny application (Figure 3). This application allows researchers to specify their rating design and explore the relationship between various IRR metrics and PRA, superimpose multiple recommendations (Table 2), and predict ICC values from PRA.</p>
</div>
<div id="references" class="section level1">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<p>Altman, D. G. (1990). <em>Practical statistics for medical research.</em> London: Chapman &amp; Hall/CRC press.</p>
<p>American Educational Research Association, American Psychological Association, &amp; National Council on Measurement in Education, &amp; Joint Committee on Standards for Educational and Psychological Testing. (2014). <em>Standards for educational and psychological testing.</em> Washington, DC: AERA.</p>
<p>Brage, M. E., Rockett, M., Vraney, R., Anderson, R., &amp; Toledano, A. (1998). Ankle fracture classification: A comparison of reliability of three x-ray views versus two. <em>Foot &amp; Ankle International, 19</em>(8), 555-562.</p>
<p>Cicchetti, D. V. (2001). Methodological Commentary. The precision of reliability and validity estimates re-visited: Distinguishing between clinical and statistical significance of sample size requirements. <em>Journal of Clinical and Experimental Neuropsychology, 23</em>(5), 695- 700.</p>
<p>Cicchetti, D. V., &amp; Sparrow, S. A. (1981). Developing criteria for establishing interrater reliability of specific items: Applications to assessment of adaptive behavior. <em>American Journal of Mental Deficiency, 86</em>(2), 127-137.</p>
<p>Fleiss, J. L. (1981). <em>Statistical methods for rates and proportions.</em> New York: John Wiley &amp; Sons.</p>
<p>Fleiss, J. L. (1986). The design and analysis of clinical experiments. New York: Wiley. Hallgren, K. A. (2012). Computing inter-rater reliability for observational data: An overview and tutorial. <em>Tutorials in Quantitative Methods for Psychology, 8</em>(1), 23-34.</p>
<p>Koo, T. K., &amp; Li, M. Y. (2016). A guideline of selecting and reporting intraclass correlation coefficients for reliability research. <em>Journal of Chiropractic Medicine, 15</em>(2), 155-163.</p>
<p>Kottner, J., Audigé, L., Brorson, S., Donner, A., Gajewski, B. J., Hróbjartsson, A., … &amp; Streiner, D. L. (2011). Guidelines for reporting reliability and agreement studies (GRRAS) were proposed. <em>International Journal of Nursing Studies, 48</em>(6), 661-671.</p>
<p>Landis, J. R., &amp; Koch, G. G. (1977). The measurement of observer agreement for categorical data. <em>Biometrics, 33</em>(1), 159-174.</p>
<p>Martin, J. S., Marsh, J. L., Bonar, S. K., DeCoster, T. A., Found, E. M., &amp; Brandser, E. A. (1997). Assessment of the AO/ASIF fracture classification for the distal tibia. <em>Journal of Orthopaedic Trauma, 11</em>(7), 477-483.</p>
<p>McGraw, K. O., &amp; Wong, S. P. (1996). Forming inferences about some intraclass correlation coefficients. <em>Psychological Methods, 1</em>(1), 30-46.</p>
<p>Portney, L. G., &amp; Watkins, M. P. (2009). <em>Foundations of clinical research: applications to practice.</em> Upper Saddle River: Pearson/Prentice Hall.</p>
<p>Shrout, P. E. (1998). Measurement reliability and agreement in psychiatry. <em>Statistical Methods in Medical Research, 7</em>(3), 301-317.</p>
<p>Shrout, P. E., &amp; Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing rater reliability. <em>Psychological Bulletin, 86</em>(2), 420-428.</p>
<p>Svanholm, H., Starklint, H., Gundersen, H. J. G., Fabricius, J., Barlebo, H., &amp; Olsen, S. (1989). Reproducibility of histomorphologic diagnoses with special reference to the kappa statistic. <em>Apmis, 97</em>(7-12), 689-698.</p>
<p>Trevethan, R. (2017). Intraclass correlation coefficients: Clearing the air, extending some cautions, and making some requests. <em>Health Services and Outcomes Research Methodology, 17</em>(2), 127-143.</p>
<p>Zegers, M., de Bruijne, M.C., Wagner, C., Groenewegen, P.P., van der Wal, G., de Vet, H.C. (2010). The inter-rater agreement of retrospective assessments of adverse events does not improve with two reviewers per patient record. <em>Journal of Clinical Epidemiology, 63</em>(1), 94–102.</p>
<style type="text/css">
p {
    font-style: normal;
}
</style>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#research-questions">Research Questions</a></li>
      <li><a href="#method">Method</a></li>
      <li><a href="#discussion">Discussion</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Jason Bryer.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
